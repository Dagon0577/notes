# 学习内容 2019-06-18
- [VIP概念](#VIP概念)
- [高可用](#高可用)
- [多网卡绑定的概念](#多网卡绑定的概念)
- [keepalived](#keepalived)
- [LVS](https://blog.csdn.net/weixin_40470303/article/details/80541639)
## VIP概念

- VIP。虚拟IP，就是一个未分配给真实主机的IP，也就是说对外提供数据库服务器的主机除了有一个真实IP外还有一个虚IP，使用这两个IP中的 任意一个都可以连接到这台主机，所有项目中数据库链接一项配置的都是这个虚IP，当服务器发生故障无法对外提供服务时，动态将这个虚IP切换到备用主机。
其实现原理主要是靠TCP/IP的ARP协议。因为ip地址只是一个逻辑 地址，在以太网中MAC地址才是真正用来进行数据传输的物理地址，每台主机中都有一个ARP高速缓存，存储同一个网络内的IP地址与MAC地址的对应关 系，以太网中的主机发送数据时会先从这个缓存中查询目标IP对应的MAC地址，会向这个MAC地址发送数据。操作系统会自动维护这个缓存。这就是整个实现 的关键。

        下边就是我电脑上的arp缓存的内容。

        (192.168.1.219) at 00:21:5A:DB:68:E8 [ether] on bond0
        (192.168.1.217) at 00:21:5A:DB:68:E8 [ether] on bond0
        (192.168.1.218) at 00:21:5A:DB:7F:C2 [ether] on bond0

        192.168.1.217、192.168.1.218是两台真实的电脑，
        192.168.1.217为对外提供数据库服务的主机。
        192.168.1.218为热备的机器。
        192.168.1.217为虚IP。
        大家注意红字部分，219、217的MAC地址是相同的。
        再看看那217宕机后的arp缓存

        (192.168.1.219) at 00:21:5A:DB:7F:C2 [ether] on bond0
        (192.168.1.217) at 00:21:5A:DB:68:E8 [ether] on bond0
        (192.168.1.218) at 00:21:5A:DB:7F:C2 [ether] on bond0

        这就是奥妙所在。
        当218 发现217宕机后会向网络发送一个ARP数据包，告诉所有主机192.168.1.219这个IP对应的MAC地址是00:21:5A:DB:7F:C2 ，这样所有发送到219的数据包都会发送到mac地址为00:21:5A:DB:7F:C2的机器


## 高可用
- 高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。

假设系统一直能够提供服务，我们说系统的可用性是100%。

如果系统每运行100个时间单位，会有1个时间单位无法提供服务，我们说系统的可用性是99%。

很多公司的高可用目标是4个9，也就是99.99%，这就意味着，系统的年停机时间为8.76个小时。

    高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。

    方法论上，高可用是通过冗余+自动故障转移来实现的。

    整个互联网分层系统架构的高可用，又是通过每一层的冗余+自动故障转移来综合实现的，具体的：

    （1）【客户端层】到【反向代理层】的高可用，是通过反向代理层的冗余实现的，常见实践是keepalived + virtual IP自动故障转移

    （2）【反向代理层】到【站点层】的高可用，是通过站点层的冗余实现的，常见实践是nginx与web-server之间的存活性探测与自动故障转移

    （3）【站点层】到【服务层】的高可用，是通过服务层的冗余实现的，常见实践是通过service-connection-pool来保证自动故障转移

    （4）【服务层】到【缓存层】的高可用，是通过缓存数据的冗余实现的，常见实践是缓存客户端双读双写，或者利用缓存集群的主从数据同步与sentinel保活与自动故障转移；更多的业务场景，对缓存没有高可用要求，可以使用缓存服务化来对调用方屏蔽底层复杂性

    （5）【服务层】到【数据库“读”】的高可用，是通过读库的冗余实现的，常见实践是通过db-connection-pool来保证自动故障转移

    （6）【服务层】到【数据库“写”】的高可用，是通过写库的冗余实现的，常见实践是keepalived + virtual IP自动故障转移

## 多网卡绑定的概念

- 多网卡绑定一方面能够提高网络吞吐量，另一方面也可以增强网络高可用。
从软件的角度来看，多网卡绑定实际上只需要提供一个额外的bond驱动程序即可，通过该虚拟网卡驱动程序可以将实际多块网卡屏蔽，对TCP/IP协议层而言只存在一个Bond网卡。

- Linux网卡绑定共七种模式，分别是如下模式：
    - mode=0  round-robin轮询策略(Round-robin policy)

    - mode=1  active-backup主备策略(Active-backup policy)

    - mode=2  load balancing (xor)异或策略(XOR policy)
    - mode=3  fault-tolerance (broadcast)广播策略(Broadcast policy)

    - mode=4  lacp IEEE 802.3ad 动态链路聚合(IEEE 802.3ad Dynamic link aggregation)

    - mode=5  transmit load balancing适配器传输负载均衡(Adaptive transmit load balancing) 
    - mode=6  adaptive load balancing适配器负载均衡(Adaptive load balancing)

1.round-robin轮询策略  

    cat /proc/net/bonding/bond0
    Bonding Mode: load balancing (round-robin)

该模式下，链路处于负载均衡状态，数据以轮询方式向每条链路发送报文，基于per packet方式发送。即每条链路各一个数据包。这模式好处在于增加了带宽，同时支持容错能力，当有链路出问题，会把流量切换到正常的链路上。该模式下，交换机端需要配置聚合口，在cisco交换机上叫port channel

2.active-backup主备策略

    cat /proc/net/bonding/bond0
    Bonding Mode: fault-tolerance (active-backup) 

在该模式下，一个端口处于主状态，一个处于备状态，所有流量都在主链路上发出和接收，备链路不会有任何流量。当主端口down掉时，备端口接管主状态。同时可以设置primary网卡，若primary网卡出现故障，切换至备网卡，primary网卡回复后，流量自动回切。这种模式接入不需要交换机端支持。

3.load balancing (xor)异或策略

    cat /proc/net/bonding/bond0
    Bonding Mode: load balancing (xor) 
在该模式下，通过源和目标mac做hash因子来做xor算法来选择链路，这样就使得到达特定对端的流量总是从同一个接口上发出。和balance-rr一样，交换机端口需要能配置为“port channel”。
值得注意的是，若选择这种模式，如果所有流量源和目标mac都固定了，例如使用“网关模式”，即所有对外的数据传输均固定走一个网关，那么根据该模式的描述，分发算法算出的线路就一直是同一条，另外一条链路不会有任何数据流，那么这种模式就没有多少意义了。

4.fault-tolerance (broadcast)广播策略

    cat /proc/net/bonding/bond0
    Bonding Mode: fault-tolerance (broadcast)
这种模式的特点是一个报文会复制两份往bond下的两个接口分别发送出去。当有对端交换机失效，我们感觉不到任何丢包。这个模式也需要交换机配置聚合口。这种模式的特点是，同一个报文服务器会复制两份分别往两条线路发送，导致回复两份重复报文，虽然这种模式不能起到增加网络带宽的效果，反而给网络增加负担，但对于一些需要高可用的环境下，例如RAC的心跳网络，还是有一定价值的。

5.lacp IEEE 802.3ad 动态链路聚合

该模式拓扑结构与主备模式相同  

    cat /proc/net/bonding/bond0
    Bonding Mode: IEEE 802.3ad Dynamic link aggregation
该模式是基于IEEE 802.3ad Dynamic link aggregation（动态链接聚合）协议，针对该协议的介绍，在公众号之前的文章中有所涉及。
在该模式下，操作系统和交换机都会创建一个聚合组，在同一聚合组下的网口共享同样的速率和双工设定。操作系统根据802.3ad 协议将多个slave 网卡绑定在一个聚合组下。聚合组向外发送数据选择哪一块儿网卡是基于传输hash 策略，该策略可以通过xmit_hash_policy 选项从缺省的XOR 策略改变到其他策略。
该模式的必要条件：
- ethtool 支持获取每个slave 的速率和双工设定；
- 交换机支持IEEE 802.3ad Dynamic link aggregation。
大多数交换机需要经过特定配置才能支持802.3ad 模式。

6.transmit load balancing适配器传输负载均衡

    cat /proc/net/bonding/bond0
    Bonding Mode: transmit load balancing
这种模式相较load balancing (xor)异或策略及LACP模式的hash策略相对智能，会主动根据对端的MAC地址上的流量，智能的分配流量从哪个网卡发出。但不足之处在于，仍使用一块网卡接收数据。存在的问题与load balancing (xor)也是一样的一样，如果对端MAC地址是唯一的，那么策略就会失效。这个模式下bond成员使用各自的mac，而不是上面几种模式是使用bond0接口的mac。无需交换机支持

7.adaptive load balancing适配器负载均衡

    cat /proc/net/bonding/bond0
    Bonding Mode: adaptive load balancing
该模式除了balance-tlb适配器传输负载均衡模式的功能外，同时加上针对IPV4流量接收的负载均衡。接收负载均衡是通过ARP协商实现的。在进行ARP协商的过程中，bond模块将对端和本地的mac地址进行绑定，这样从同一端发出的数据，在本地也会一直使用同一块网卡来接收。若是网络上发出的广播包，则由不同网卡轮询的方式来进行接收。通过这种方式实现了接收的负载均衡。该模式同样无需交换机支持。


## keepalived
- keepalived的作用是检测后端TCP服务的状态，如果有一台提供TCP服务的后端节点死机，或者工作出现故障，keepalived会及时检测到，并将有故障的节点从系统中剔除，当提供TCP服务的节点恢复并且正常提供服务后keepalived会自动将TCP服务的节点加入到集群中。这些工作都是keepalived自动完成，不需要人工干涉，需要人工做的只是修复发生故障的服务器。

一、两台http服务器的安装

　　1、  两台机均安装httpd

    $ sudo yum install -y httpd

2、  添加首页

    $ sudo -i
    
    #http服务器1设置
    # echo “10.0.0.22” >/var/www/html/index.html
    
    #http服务器2设置
    # echo “10.0.0.23” >/var/www/html/index.html

3、  启动并设置开机启动httpd

    $ sudo systemctl start httpd
    $ sudo systemctl enable httpd

二、两台keepalived主机的设置

　　1、  两台机均安装keepalived

    #安装依赖文件与keepalive
    
    $ sudo yum install -y openssl openssl-devel keepalived


    2、  keepalived主机配置

    $ sudo vim /etc/keepalived/keepalived.conf
 
    vrrp_instance VI_1 {
    #指定该节点为主节点，备用节点设置为BACKUP
    state MASTER
 
    #绑定虚拟IP的网络接口
    interface eno16777736
 
    #VRRP组名，两个节点设置一样，以指明各个节点同属一VRRP组
    virtual_router_id 51
 
    #主节点的优先级，数值在1~254，注意从节点必须比主节点的优先级别低
    priority 50
 
    ##组播信息发送间隔，两个节点需一致
    advert_int 1
 
    #设置验证信息，两个节点需一致
    authentication{
      auth_type PASS
      auth_pass 1111
    }
 
    #指定虚拟IP，两个节点需设置一样
    virtual_ipaddress{
      10.0.0.100
    }
    }#虚拟IP服务
    virtual_server 10.0.0.100 80 {
        #设定检查间隔
        delay_loop 6
    
    #指定LVS算法
        lb_algo rr
    
    #指定LVS模式
        lb_kind NAT
        nat_mask 255.255.255.0
    
    #持久连接设置，会话保持时间
    persistence_timeout 50
    
    #转发协议为TCP
    protocol TCP
    
    #后端实际TCP服务配置
    real_server 10.0.0.22 80 {
        weight 1
    }
    
    real_server 10.0.0.23 80 {
        weight 1
    }
    }

3、   keepalived备机的keepalived.conf的配置，不同之处如下：

    state BACKUP
    priority 30
    
    #其它配置跟keepalived主机相同

/etc/keepalived/keepalived.conf为keepalived的主配置文件。以上配置state表示主节点为10.0.0.20，副节点为10.0.0.21。虚拟为IP10.0.0.100。后端的真实服务器为10.0.0.22和10.0.0.23，当通过10.0.0.100访问web服务器时，自动转到后端真实服务器，后端节点的权重相同，类似轮询的模式。

三、keepalived的启动与测试

　　1、  启动keepalived

    $ sudo systemctl start keepalived
    $ sudo systemctl enable keepalived
 

　　2、  查看keepalived主机的IP

    $ ip addr show